"""
Vector Database Client for CRO Analyzer

Uses ChromaDB to store and query historical CRO audit patterns.
Enables semantic search for similar issues across past audits.
"""

import hashlib
import chromadb
from typing import List, Dict, Optional
from sentence_transformers import SentenceTransformer

from config import settings


class VectorDBClient:
    """
    ChromaDB client for storing and querying CRO audit patterns.

    Schema:
    - Collection: "cro_audit_issues"
    - Documents: Issue descriptions + recommendations (full text)
    - Metadata: {
        'client_name': str,
        'section': str (Navigation, Hero, Product Page, etc.),
        'issue_title': str,
        'why_it_matters': str,
        'recommendations': str (joined with '; '),
        'industry': str (e-commerce, B2B, SaaS, etc.),
        'audit_date': str (YYYY-MM-DD)
      }
    - Embeddings: Auto-generated by sentence-transformers model
    """

    def __init__(
        self,
        collection_name: str = "cro_audit_issues",
        model_name: str = "all-MiniLM-L6-v2",
    ):
        """
        Initialize ChromaDB Client (supports both Cloud and Self-Hosted).

        Args:
            collection_name: Name of the collection to use
            model_name: Sentence transformer model for embeddings
        """
        self.collection_name = collection_name
        self.model_name = model_name

        # Load sentence transformer model for embeddings
        print(f"Loading embedding model: {model_name}...")
        self.embedding_model = SentenceTransformer(model_name)
        print(f"‚úì Loaded {model_name}")

        # Initialize ChromaDB Client (auto-detect Cloud vs Self-Hosted)
        self.client = self._initialize_client()

        # Get existing collection (workaround for Railway 0.5.23 server bug)
        # The collection should already exist from data migration
        try:
            self.collection = self.client.get_collection(name=collection_name)
            print(f"‚úì Connected to existing ChromaDB collection: {collection_name}")
            print(f"  Total issues in database: {self.collection.count()}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Collection '{collection_name}' not found, attempting to create...")
            # Fallback to create if collection doesn't exist
            # Note: This will fail on Railway 0.5.23 due to server bug
            self.collection = self.client.create_collection(
                name=collection_name,
                metadata={"description": "CRO audit issues and recommendations"},
            )
            print(f"‚úì Created new ChromaDB collection: {collection_name}")
            print(f"  Total issues in database: {self.collection.count()}")

    def _initialize_client(self):
        """
        Initialize ChromaDB client based on environment configuration.
        Supports both Cloud (legacy) and Self-Hosted (preferred) deployments.

        Returns:
            ChromaDB client instance (CloudClient or HttpClient)
        """
        # Check for self-hosted configuration (preferred)
        chroma_host = settings.CHROMA_HOST

        if chroma_host:
            # Self-hosted deployment
            chroma_port = settings.CHROMA_PORT
            chroma_ssl = settings.CHROMA_SSL

            print(f"Connecting to self-hosted ChromaDB:")
            print(f"  Host: {chroma_host}")
            print(f"  Port: {chroma_port}")
            print(f"  SSL: {chroma_ssl}")

            # Build headers with optional auth token
            headers = {}
            auth_token = settings.CHROMA_AUTH_TOKEN
            if auth_token:
                headers["Authorization"] = f"Bearer {auth_token}"
                print(f"  Auth: Token provided")

            return chromadb.HttpClient(
                host=chroma_host,
                port=chroma_port,
                ssl=chroma_ssl,
                headers=headers if headers else None,
            )

        else:
            # Cloud deployment (legacy)
            print("Connecting to ChromaDB Cloud (legacy)...")
            return chromadb.CloudClient(
                tenant=settings.CHROMA_TENANT,
                database=settings.CHROMA_DATABASE,
                api_key=settings.CHROMA_API_KEY,
            )

    def add_issue(
        self,
        client_name: str,
        section: str,
        issue_title: str,
        issue_description: str,
        why_it_matters: str,
        recommendations: List[str],
        industry: str = "unknown",
        audit_date: str = "2024-01-01",
    ) -> str:
        """
        Add a single CRO issue to the vector database.

        Args:
            client_name: Name of the client
            section: Section name (Navigation, Hero, Product Page, etc.)
            issue_title: Brief title of the issue
            issue_description: Detailed description of what's wrong
            why_it_matters: Rationale for why this issue impacts conversions
            recommendations: List of actionable recommendations
            industry: Industry type (e-commerce, B2B, SaaS, etc.)
            audit_date: Date of audit in YYYY-MM-DD format

        Returns:
            ID of the added document
        """
        # Generate document ID
        doc_id = f"{client_name}_{section}_{issue_title}".replace(" ", "_").replace(
            "/", "-"
        )[:100]

        # Combine text for embedding
        full_text = f"{issue_title}. {issue_description} {why_it_matters}"
        recommendations_text = "; ".join(recommendations)

        # Generate embedding
        embedding = self.embedding_model.encode(full_text).tolist()

        # Add to collection
        self.collection.add(
            ids=[doc_id],
            embeddings=[embedding],
            documents=[full_text],
            metadatas=[
                {
                    "client_name": client_name,
                    "section": section,
                    "issue_title": issue_title,
                    "issue_description": issue_description,
                    "why_it_matters": why_it_matters,
                    "recommendations": recommendations_text,
                    "industry": industry,
                    "audit_date": audit_date,
                }
            ],
        )

        return doc_id

    def add_issues_bulk(self, issues: List[Dict]) -> int:
        """
        Add multiple issues to the database in bulk.

        Args:
            issues: List of issue dictionaries, each containing:
                - client_name, section, issue_title, issue_description,
                  why_it_matters, recommendations, industry, audit_date

        Returns:
            Number of issues added
        """
        ids = []
        embeddings = []
        documents = []
        metadatas = []

        for idx, issue in enumerate(issues):
            # Generate unique document ID using content hash + counter to avoid collisions
            # Include counter in hash to ensure uniqueness even with identical content
            content_for_hash = f"{issue['client_name']}_{issue['section']}_{issue['issue_title']}_{issue.get('issue_description', '')}_{idx}"
            hash_suffix = hashlib.md5(content_for_hash.encode()).hexdigest()[:8]

            # Create ID with truncated fields + hash suffix (stays under 100 chars)
            client_part = issue["client_name"][:30].replace(" ", "_").replace("/", "-")
            section_part = issue["section"][:20].replace(" ", "_").replace("/", "-")
            doc_id = f"{client_part}_{section_part}_{hash_suffix}"

            # Combine text for embedding
            full_text = f"{issue['issue_title']}. {issue['issue_description']} {issue.get('why_it_matters', '')}"

            # Generate embedding
            embedding = self.embedding_model.encode(full_text).tolist()

            # Prepare data
            recommendations_text = "; ".join(issue.get("recommendations", []))

            ids.append(doc_id)
            embeddings.append(embedding)
            documents.append(full_text)
            metadatas.append(
                {
                    "client_name": issue["client_name"],
                    "section": issue["section"],
                    "issue_title": issue["issue_title"],
                    "issue_description": issue.get("issue_description", ""),
                    "why_it_matters": issue.get("why_it_matters", ""),
                    "recommendations": recommendations_text,
                    "industry": issue.get("industry", "unknown"),
                    "audit_date": issue.get("audit_date", "2024-01-01"),
                }
            )

        # Bulk add to collection
        self.collection.add(
            ids=ids, embeddings=embeddings, documents=documents, metadatas=metadatas
        )

        print(f"‚úì Added {len(issues)} issues to vector database")
        return len(issues)

    def query_similar_issues(
        self,
        query_text: str,
        section: Optional[str] = None,
        industry: Optional[str] = None,
        n_results: int = 5,
    ) -> List[Dict]:
        """
        Query for similar CRO issues using semantic search.

        Args:
            query_text: The issue description to search for
            section: Optional section filter (Navigation, Hero, etc.)
            industry: Optional industry filter (e-commerce, B2B, etc.)
            n_results: Number of similar issues to return

        Returns:
            List of similar issues with metadata and similarity scores
        """
        # Generate embedding for query
        query_embedding = self.embedding_model.encode(query_text).tolist()

        # Build where filter
        where_filter = {}
        if section:
            where_filter["section"] = section
        if industry:
            where_filter["industry"] = industry

        # Query collection
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results,
            where=where_filter if where_filter else None,
            include=["documents", "metadatas", "distances"],
        )

        # Format results
        similar_issues = []
        for i in range(len(results["ids"][0])):
            similar_issues.append(
                {
                    "id": results["ids"][0][i],
                    "similarity": 1
                    - results["distances"][0][i],  # Convert distance to similarity
                    "text": results["documents"][0][i],
                    "metadata": results["metadatas"][0][i],
                }
            )

        return similar_issues

    def get_section_patterns(self, section: str, top_k: int = 10) -> List[Dict]:
        """
        Get the most common issues for a specific section across all audits.

        Args:
            section: Section name (Navigation, Hero, Product Page, etc.)
            top_k: Number of top patterns to return

        Returns:
            List of common issues for the section
        """
        # Get all issues for the section
        results = self.collection.get(
            where={"section": section}, limit=top_k, include=["documents", "metadatas"]
        )

        patterns = []
        for i in range(len(results["ids"])):
            patterns.append(
                {
                    "id": results["ids"][i],
                    "text": results["documents"][i],
                    "metadata": results["metadatas"][i],
                }
            )

        return patterns

    def get_stats(self) -> Dict[str, int]:
        """
        Get database statistics.

        Returns:
            Dictionary with counts by section, industry, etc.
        """
        all_issues = self.collection.get(include=["metadatas"])
        metadatas = all_issues["metadatas"]

        # Count by section
        section_counts = {}
        industry_counts = {}

        for metadata in metadatas:
            section = metadata.get("section", "Unknown")
            industry = metadata.get("industry", "unknown")

            section_counts[section] = section_counts.get(section, 0) + 1
            industry_counts[industry] = industry_counts.get(industry, 0) + 1

        return {
            "total_issues": len(metadatas),
            "sections": section_counts,
            "industries": industry_counts,
        }

    def clear_collection(self):
        """Clear all data from the collection (use with caution!)."""
        self.client.delete_collection(self.collection_name)
        self.collection = self.client.create_collection(
            name=self.collection_name,
            metadata={"description": "CRO audit issues and recommendations"},
        )
        print(f"‚úì Cleared collection: {self.collection_name}")


# Usage example
if __name__ == "__main__":
    # Initialize client
    db = VectorDBClient()

    # Add a sample issue
    db.add_issue(
        client_name="Test Client",
        section="Navigation",
        issue_title="Complex navigation structure",
        issue_description="The current navigation has too many nested levels making it difficult for users to find products.",
        why_it_matters="Complex navigation increases cognitive load and leads to higher bounce rates and lower conversion rates.",
        recommendations=[
            "Simplify to 2-level navigation maximum",
            "Add mega menu with visual categories",
            "Include search bar prominently",
        ],
        industry="e-commerce",
        audit_date="2024-01-15",
    )

    # Query for similar issues
    similar = db.query_similar_issues(
        query_text="Navigation is too complicated and confusing",
        section="Navigation",
        n_results=3,
    )

    print(f"\nüîç Similar Issues Found:")
    for issue in similar:
        print(f"\nSimilarity: {issue['similarity']:.2%}")
        print(f"Title: {issue['metadata']['issue_title']}")
        print(f"Client: {issue['metadata']['client_name']}")
        print(f"Recommendations: {issue['metadata']['recommendations']}")

    # Get stats
    stats = db.get_stats()
    print(f"\nüìä Database Stats:")
    print(f"Total Issues: {stats['total_issues']}")
    print(f"Sections: {stats['sections']}")
    print(f"Industries: {stats['industries']}")
